{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.python.keras.utils.np_utils import to_categorical\n",
    "from keras import models\n",
    "from keras import layers"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Взято с https://machinelearningmastery.ru/standard-machine-learning-datasets/\n",
    "Описан пример с показанием сонара sonar.csv\n",
    "по аналогии с ним (с переименовыванием столбцов классов) можно сделать датасет Ionosphere.csv ; seeds_dataset.csv (там классы 1,2,3, а нужны 0,1,2)\n",
    "\n",
    "по аналогии (но без переименовывания столбцов класса) датасеты: pima-indians-diabetes.csv ; data_banknote_authentication.csv\n",
    "\n",
    "по аналогии с датасетом ирисов можно сделать:\n",
    "1. from sklearn.datasets import load_wine\n",
    "2. from sklearn.datasets import load_breast_cancer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "     attribute_1  attribute_2  attribute_3  attribute_4  attribute_5  \\\n0         0.0200       0.0371       0.0428       0.0207       0.0954   \n1         0.0453       0.0523       0.0843       0.0689       0.1183   \n2         0.0262       0.0582       0.1099       0.1083       0.0974   \n3         0.0100       0.0171       0.0623       0.0205       0.0205   \n4         0.0762       0.0666       0.0481       0.0394       0.0590   \n..           ...          ...          ...          ...          ...   \n203       0.0187       0.0346       0.0168       0.0177       0.0393   \n204       0.0323       0.0101       0.0298       0.0564       0.0760   \n205       0.0522       0.0437       0.0180       0.0292       0.0351   \n206       0.0303       0.0353       0.0490       0.0608       0.0167   \n207       0.0260       0.0363       0.0136       0.0272       0.0214   \n\n     attribute_6  attribute_7  attribute_8  attribute_9  attribute_10  ...  \\\n0         0.0986       0.1539       0.1601       0.3109        0.2111  ...   \n1         0.2583       0.2156       0.3481       0.3337        0.2872  ...   \n2         0.2280       0.2431       0.3771       0.5598        0.6194  ...   \n3         0.0368       0.1098       0.1276       0.0598        0.1264  ...   \n4         0.0649       0.1209       0.2467       0.3564        0.4459  ...   \n..           ...          ...          ...          ...           ...  ...   \n203       0.1630       0.2028       0.1694       0.2328        0.2684  ...   \n204       0.0958       0.0990       0.1018       0.1030        0.2154  ...   \n205       0.1171       0.1257       0.1178       0.1258        0.2529  ...   \n206       0.1354       0.1465       0.1123       0.1945        0.2354  ...   \n207       0.0338       0.0655       0.1400       0.1843        0.2354  ...   \n\n     attribute_52  attribute_53  attribute_54  attribute_55  attribute_56  \\\n0          0.0027        0.0065        0.0159        0.0072        0.0167   \n1          0.0084        0.0089        0.0048        0.0094        0.0191   \n2          0.0232        0.0166        0.0095        0.0180        0.0244   \n3          0.0121        0.0036        0.0150        0.0085        0.0073   \n4          0.0031        0.0054        0.0105        0.0110        0.0015   \n..            ...           ...           ...           ...           ...   \n203        0.0116        0.0098        0.0199        0.0033        0.0101   \n204        0.0061        0.0093        0.0135        0.0063        0.0063   \n205        0.0160        0.0029        0.0051        0.0062        0.0089   \n206        0.0086        0.0046        0.0126        0.0036        0.0035   \n207        0.0146        0.0129        0.0047        0.0039        0.0061   \n\n     attribute_57  attribute_58  attribute_59  attribute_60  Class  \n0          0.0180        0.0084        0.0090        0.0032   Rock  \n1          0.0140        0.0049        0.0052        0.0044   Rock  \n2          0.0316        0.0164        0.0095        0.0078   Rock  \n3          0.0050        0.0044        0.0040        0.0117   Rock  \n4          0.0072        0.0048        0.0107        0.0094   Rock  \n..            ...           ...           ...           ...    ...  \n203        0.0065        0.0115        0.0193        0.0157   Mine  \n204        0.0034        0.0032        0.0062        0.0067   Mine  \n205        0.0140        0.0138        0.0077        0.0031   Mine  \n206        0.0034        0.0079        0.0036        0.0048   Mine  \n207        0.0040        0.0036        0.0061        0.0115   Mine  \n\n[208 rows x 61 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>attribute_1</th>\n      <th>attribute_2</th>\n      <th>attribute_3</th>\n      <th>attribute_4</th>\n      <th>attribute_5</th>\n      <th>attribute_6</th>\n      <th>attribute_7</th>\n      <th>attribute_8</th>\n      <th>attribute_9</th>\n      <th>attribute_10</th>\n      <th>...</th>\n      <th>attribute_52</th>\n      <th>attribute_53</th>\n      <th>attribute_54</th>\n      <th>attribute_55</th>\n      <th>attribute_56</th>\n      <th>attribute_57</th>\n      <th>attribute_58</th>\n      <th>attribute_59</th>\n      <th>attribute_60</th>\n      <th>Class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0200</td>\n      <td>0.0371</td>\n      <td>0.0428</td>\n      <td>0.0207</td>\n      <td>0.0954</td>\n      <td>0.0986</td>\n      <td>0.1539</td>\n      <td>0.1601</td>\n      <td>0.3109</td>\n      <td>0.2111</td>\n      <td>...</td>\n      <td>0.0027</td>\n      <td>0.0065</td>\n      <td>0.0159</td>\n      <td>0.0072</td>\n      <td>0.0167</td>\n      <td>0.0180</td>\n      <td>0.0084</td>\n      <td>0.0090</td>\n      <td>0.0032</td>\n      <td>Rock</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0453</td>\n      <td>0.0523</td>\n      <td>0.0843</td>\n      <td>0.0689</td>\n      <td>0.1183</td>\n      <td>0.2583</td>\n      <td>0.2156</td>\n      <td>0.3481</td>\n      <td>0.3337</td>\n      <td>0.2872</td>\n      <td>...</td>\n      <td>0.0084</td>\n      <td>0.0089</td>\n      <td>0.0048</td>\n      <td>0.0094</td>\n      <td>0.0191</td>\n      <td>0.0140</td>\n      <td>0.0049</td>\n      <td>0.0052</td>\n      <td>0.0044</td>\n      <td>Rock</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0262</td>\n      <td>0.0582</td>\n      <td>0.1099</td>\n      <td>0.1083</td>\n      <td>0.0974</td>\n      <td>0.2280</td>\n      <td>0.2431</td>\n      <td>0.3771</td>\n      <td>0.5598</td>\n      <td>0.6194</td>\n      <td>...</td>\n      <td>0.0232</td>\n      <td>0.0166</td>\n      <td>0.0095</td>\n      <td>0.0180</td>\n      <td>0.0244</td>\n      <td>0.0316</td>\n      <td>0.0164</td>\n      <td>0.0095</td>\n      <td>0.0078</td>\n      <td>Rock</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0100</td>\n      <td>0.0171</td>\n      <td>0.0623</td>\n      <td>0.0205</td>\n      <td>0.0205</td>\n      <td>0.0368</td>\n      <td>0.1098</td>\n      <td>0.1276</td>\n      <td>0.0598</td>\n      <td>0.1264</td>\n      <td>...</td>\n      <td>0.0121</td>\n      <td>0.0036</td>\n      <td>0.0150</td>\n      <td>0.0085</td>\n      <td>0.0073</td>\n      <td>0.0050</td>\n      <td>0.0044</td>\n      <td>0.0040</td>\n      <td>0.0117</td>\n      <td>Rock</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0762</td>\n      <td>0.0666</td>\n      <td>0.0481</td>\n      <td>0.0394</td>\n      <td>0.0590</td>\n      <td>0.0649</td>\n      <td>0.1209</td>\n      <td>0.2467</td>\n      <td>0.3564</td>\n      <td>0.4459</td>\n      <td>...</td>\n      <td>0.0031</td>\n      <td>0.0054</td>\n      <td>0.0105</td>\n      <td>0.0110</td>\n      <td>0.0015</td>\n      <td>0.0072</td>\n      <td>0.0048</td>\n      <td>0.0107</td>\n      <td>0.0094</td>\n      <td>Rock</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>203</th>\n      <td>0.0187</td>\n      <td>0.0346</td>\n      <td>0.0168</td>\n      <td>0.0177</td>\n      <td>0.0393</td>\n      <td>0.1630</td>\n      <td>0.2028</td>\n      <td>0.1694</td>\n      <td>0.2328</td>\n      <td>0.2684</td>\n      <td>...</td>\n      <td>0.0116</td>\n      <td>0.0098</td>\n      <td>0.0199</td>\n      <td>0.0033</td>\n      <td>0.0101</td>\n      <td>0.0065</td>\n      <td>0.0115</td>\n      <td>0.0193</td>\n      <td>0.0157</td>\n      <td>Mine</td>\n    </tr>\n    <tr>\n      <th>204</th>\n      <td>0.0323</td>\n      <td>0.0101</td>\n      <td>0.0298</td>\n      <td>0.0564</td>\n      <td>0.0760</td>\n      <td>0.0958</td>\n      <td>0.0990</td>\n      <td>0.1018</td>\n      <td>0.1030</td>\n      <td>0.2154</td>\n      <td>...</td>\n      <td>0.0061</td>\n      <td>0.0093</td>\n      <td>0.0135</td>\n      <td>0.0063</td>\n      <td>0.0063</td>\n      <td>0.0034</td>\n      <td>0.0032</td>\n      <td>0.0062</td>\n      <td>0.0067</td>\n      <td>Mine</td>\n    </tr>\n    <tr>\n      <th>205</th>\n      <td>0.0522</td>\n      <td>0.0437</td>\n      <td>0.0180</td>\n      <td>0.0292</td>\n      <td>0.0351</td>\n      <td>0.1171</td>\n      <td>0.1257</td>\n      <td>0.1178</td>\n      <td>0.1258</td>\n      <td>0.2529</td>\n      <td>...</td>\n      <td>0.0160</td>\n      <td>0.0029</td>\n      <td>0.0051</td>\n      <td>0.0062</td>\n      <td>0.0089</td>\n      <td>0.0140</td>\n      <td>0.0138</td>\n      <td>0.0077</td>\n      <td>0.0031</td>\n      <td>Mine</td>\n    </tr>\n    <tr>\n      <th>206</th>\n      <td>0.0303</td>\n      <td>0.0353</td>\n      <td>0.0490</td>\n      <td>0.0608</td>\n      <td>0.0167</td>\n      <td>0.1354</td>\n      <td>0.1465</td>\n      <td>0.1123</td>\n      <td>0.1945</td>\n      <td>0.2354</td>\n      <td>...</td>\n      <td>0.0086</td>\n      <td>0.0046</td>\n      <td>0.0126</td>\n      <td>0.0036</td>\n      <td>0.0035</td>\n      <td>0.0034</td>\n      <td>0.0079</td>\n      <td>0.0036</td>\n      <td>0.0048</td>\n      <td>Mine</td>\n    </tr>\n    <tr>\n      <th>207</th>\n      <td>0.0260</td>\n      <td>0.0363</td>\n      <td>0.0136</td>\n      <td>0.0272</td>\n      <td>0.0214</td>\n      <td>0.0338</td>\n      <td>0.0655</td>\n      <td>0.1400</td>\n      <td>0.1843</td>\n      <td>0.2354</td>\n      <td>...</td>\n      <td>0.0146</td>\n      <td>0.0129</td>\n      <td>0.0047</td>\n      <td>0.0039</td>\n      <td>0.0061</td>\n      <td>0.0040</td>\n      <td>0.0036</td>\n      <td>0.0061</td>\n      <td>0.0115</td>\n      <td>Mine</td>\n    </tr>\n  </tbody>\n</table>\n<p>208 rows × 61 columns</p>\n</div>"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Нейронная сеть на данных сонаров\n",
    "sonar = pd.read_csv('sonar.csv')\n",
    "sonar"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "     attribute_1  attribute_2  attribute_3  attribute_4  attribute_5  \\\n0         0.0200       0.0371       0.0428       0.0207       0.0954   \n1         0.0453       0.0523       0.0843       0.0689       0.1183   \n2         0.0262       0.0582       0.1099       0.1083       0.0974   \n3         0.0100       0.0171       0.0623       0.0205       0.0205   \n4         0.0762       0.0666       0.0481       0.0394       0.0590   \n..           ...          ...          ...          ...          ...   \n203       0.0187       0.0346       0.0168       0.0177       0.0393   \n204       0.0323       0.0101       0.0298       0.0564       0.0760   \n205       0.0522       0.0437       0.0180       0.0292       0.0351   \n206       0.0303       0.0353       0.0490       0.0608       0.0167   \n207       0.0260       0.0363       0.0136       0.0272       0.0214   \n\n     attribute_6  attribute_7  attribute_8  attribute_9  attribute_10  ...  \\\n0         0.0986       0.1539       0.1601       0.3109        0.2111  ...   \n1         0.2583       0.2156       0.3481       0.3337        0.2872  ...   \n2         0.2280       0.2431       0.3771       0.5598        0.6194  ...   \n3         0.0368       0.1098       0.1276       0.0598        0.1264  ...   \n4         0.0649       0.1209       0.2467       0.3564        0.4459  ...   \n..           ...          ...          ...          ...           ...  ...   \n203       0.1630       0.2028       0.1694       0.2328        0.2684  ...   \n204       0.0958       0.0990       0.1018       0.1030        0.2154  ...   \n205       0.1171       0.1257       0.1178       0.1258        0.2529  ...   \n206       0.1354       0.1465       0.1123       0.1945        0.2354  ...   \n207       0.0338       0.0655       0.1400       0.1843        0.2354  ...   \n\n     attribute_52  attribute_53  attribute_54  attribute_55  attribute_56  \\\n0          0.0027        0.0065        0.0159        0.0072        0.0167   \n1          0.0084        0.0089        0.0048        0.0094        0.0191   \n2          0.0232        0.0166        0.0095        0.0180        0.0244   \n3          0.0121        0.0036        0.0150        0.0085        0.0073   \n4          0.0031        0.0054        0.0105        0.0110        0.0015   \n..            ...           ...           ...           ...           ...   \n203        0.0116        0.0098        0.0199        0.0033        0.0101   \n204        0.0061        0.0093        0.0135        0.0063        0.0063   \n205        0.0160        0.0029        0.0051        0.0062        0.0089   \n206        0.0086        0.0046        0.0126        0.0036        0.0035   \n207        0.0146        0.0129        0.0047        0.0039        0.0061   \n\n     attribute_57  attribute_58  attribute_59  attribute_60  Class  \n0          0.0180        0.0084        0.0090        0.0032      0  \n1          0.0140        0.0049        0.0052        0.0044      0  \n2          0.0316        0.0164        0.0095        0.0078      0  \n3          0.0050        0.0044        0.0040        0.0117      0  \n4          0.0072        0.0048        0.0107        0.0094      0  \n..            ...           ...           ...           ...    ...  \n203        0.0065        0.0115        0.0193        0.0157      1  \n204        0.0034        0.0032        0.0062        0.0067      1  \n205        0.0140        0.0138        0.0077        0.0031      1  \n206        0.0034        0.0079        0.0036        0.0048      1  \n207        0.0040        0.0036        0.0061        0.0115      1  \n\n[208 rows x 61 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>attribute_1</th>\n      <th>attribute_2</th>\n      <th>attribute_3</th>\n      <th>attribute_4</th>\n      <th>attribute_5</th>\n      <th>attribute_6</th>\n      <th>attribute_7</th>\n      <th>attribute_8</th>\n      <th>attribute_9</th>\n      <th>attribute_10</th>\n      <th>...</th>\n      <th>attribute_52</th>\n      <th>attribute_53</th>\n      <th>attribute_54</th>\n      <th>attribute_55</th>\n      <th>attribute_56</th>\n      <th>attribute_57</th>\n      <th>attribute_58</th>\n      <th>attribute_59</th>\n      <th>attribute_60</th>\n      <th>Class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0200</td>\n      <td>0.0371</td>\n      <td>0.0428</td>\n      <td>0.0207</td>\n      <td>0.0954</td>\n      <td>0.0986</td>\n      <td>0.1539</td>\n      <td>0.1601</td>\n      <td>0.3109</td>\n      <td>0.2111</td>\n      <td>...</td>\n      <td>0.0027</td>\n      <td>0.0065</td>\n      <td>0.0159</td>\n      <td>0.0072</td>\n      <td>0.0167</td>\n      <td>0.0180</td>\n      <td>0.0084</td>\n      <td>0.0090</td>\n      <td>0.0032</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0453</td>\n      <td>0.0523</td>\n      <td>0.0843</td>\n      <td>0.0689</td>\n      <td>0.1183</td>\n      <td>0.2583</td>\n      <td>0.2156</td>\n      <td>0.3481</td>\n      <td>0.3337</td>\n      <td>0.2872</td>\n      <td>...</td>\n      <td>0.0084</td>\n      <td>0.0089</td>\n      <td>0.0048</td>\n      <td>0.0094</td>\n      <td>0.0191</td>\n      <td>0.0140</td>\n      <td>0.0049</td>\n      <td>0.0052</td>\n      <td>0.0044</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0262</td>\n      <td>0.0582</td>\n      <td>0.1099</td>\n      <td>0.1083</td>\n      <td>0.0974</td>\n      <td>0.2280</td>\n      <td>0.2431</td>\n      <td>0.3771</td>\n      <td>0.5598</td>\n      <td>0.6194</td>\n      <td>...</td>\n      <td>0.0232</td>\n      <td>0.0166</td>\n      <td>0.0095</td>\n      <td>0.0180</td>\n      <td>0.0244</td>\n      <td>0.0316</td>\n      <td>0.0164</td>\n      <td>0.0095</td>\n      <td>0.0078</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0100</td>\n      <td>0.0171</td>\n      <td>0.0623</td>\n      <td>0.0205</td>\n      <td>0.0205</td>\n      <td>0.0368</td>\n      <td>0.1098</td>\n      <td>0.1276</td>\n      <td>0.0598</td>\n      <td>0.1264</td>\n      <td>...</td>\n      <td>0.0121</td>\n      <td>0.0036</td>\n      <td>0.0150</td>\n      <td>0.0085</td>\n      <td>0.0073</td>\n      <td>0.0050</td>\n      <td>0.0044</td>\n      <td>0.0040</td>\n      <td>0.0117</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0762</td>\n      <td>0.0666</td>\n      <td>0.0481</td>\n      <td>0.0394</td>\n      <td>0.0590</td>\n      <td>0.0649</td>\n      <td>0.1209</td>\n      <td>0.2467</td>\n      <td>0.3564</td>\n      <td>0.4459</td>\n      <td>...</td>\n      <td>0.0031</td>\n      <td>0.0054</td>\n      <td>0.0105</td>\n      <td>0.0110</td>\n      <td>0.0015</td>\n      <td>0.0072</td>\n      <td>0.0048</td>\n      <td>0.0107</td>\n      <td>0.0094</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>203</th>\n      <td>0.0187</td>\n      <td>0.0346</td>\n      <td>0.0168</td>\n      <td>0.0177</td>\n      <td>0.0393</td>\n      <td>0.1630</td>\n      <td>0.2028</td>\n      <td>0.1694</td>\n      <td>0.2328</td>\n      <td>0.2684</td>\n      <td>...</td>\n      <td>0.0116</td>\n      <td>0.0098</td>\n      <td>0.0199</td>\n      <td>0.0033</td>\n      <td>0.0101</td>\n      <td>0.0065</td>\n      <td>0.0115</td>\n      <td>0.0193</td>\n      <td>0.0157</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>204</th>\n      <td>0.0323</td>\n      <td>0.0101</td>\n      <td>0.0298</td>\n      <td>0.0564</td>\n      <td>0.0760</td>\n      <td>0.0958</td>\n      <td>0.0990</td>\n      <td>0.1018</td>\n      <td>0.1030</td>\n      <td>0.2154</td>\n      <td>...</td>\n      <td>0.0061</td>\n      <td>0.0093</td>\n      <td>0.0135</td>\n      <td>0.0063</td>\n      <td>0.0063</td>\n      <td>0.0034</td>\n      <td>0.0032</td>\n      <td>0.0062</td>\n      <td>0.0067</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>205</th>\n      <td>0.0522</td>\n      <td>0.0437</td>\n      <td>0.0180</td>\n      <td>0.0292</td>\n      <td>0.0351</td>\n      <td>0.1171</td>\n      <td>0.1257</td>\n      <td>0.1178</td>\n      <td>0.1258</td>\n      <td>0.2529</td>\n      <td>...</td>\n      <td>0.0160</td>\n      <td>0.0029</td>\n      <td>0.0051</td>\n      <td>0.0062</td>\n      <td>0.0089</td>\n      <td>0.0140</td>\n      <td>0.0138</td>\n      <td>0.0077</td>\n      <td>0.0031</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>206</th>\n      <td>0.0303</td>\n      <td>0.0353</td>\n      <td>0.0490</td>\n      <td>0.0608</td>\n      <td>0.0167</td>\n      <td>0.1354</td>\n      <td>0.1465</td>\n      <td>0.1123</td>\n      <td>0.1945</td>\n      <td>0.2354</td>\n      <td>...</td>\n      <td>0.0086</td>\n      <td>0.0046</td>\n      <td>0.0126</td>\n      <td>0.0036</td>\n      <td>0.0035</td>\n      <td>0.0034</td>\n      <td>0.0079</td>\n      <td>0.0036</td>\n      <td>0.0048</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>207</th>\n      <td>0.0260</td>\n      <td>0.0363</td>\n      <td>0.0136</td>\n      <td>0.0272</td>\n      <td>0.0214</td>\n      <td>0.0338</td>\n      <td>0.0655</td>\n      <td>0.1400</td>\n      <td>0.1843</td>\n      <td>0.2354</td>\n      <td>...</td>\n      <td>0.0146</td>\n      <td>0.0129</td>\n      <td>0.0047</td>\n      <td>0.0039</td>\n      <td>0.0061</td>\n      <td>0.0040</td>\n      <td>0.0036</td>\n      <td>0.0061</td>\n      <td>0.0115</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>208 rows × 61 columns</p>\n</div>"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sonar.loc[(sonar.Class == 'Rock'), 'Class'] = 0 # замена значения столбца .Class который равен 'Rock' заменить на 0\n",
    "sonar.loc[(sonar.Class == 'Mine'), 'Class'] = 1\n",
    "sonar"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "     attribute_1  attribute_2  attribute_3  attribute_4  attribute_5  \\\n2         0.0262       0.0582       0.1099       0.1083       0.0974   \n154       0.0117       0.0069       0.0279       0.0583       0.0915   \n183       0.0096       0.0404       0.0682       0.0688       0.0887   \n85        0.0365       0.1632       0.1636       0.1421       0.1130   \n66        0.0265       0.0440       0.0137       0.0084       0.0305   \n..           ...          ...          ...          ...          ...   \n50        0.0353       0.0713       0.0326       0.0272       0.0370   \n184       0.0269       0.0383       0.0505       0.0707       0.1313   \n165       0.0221       0.0065       0.0164       0.0487       0.0519   \n7         0.0519       0.0548       0.0842       0.0319       0.1158   \n70        0.0065       0.0122       0.0068       0.0108       0.0217   \n\n     attribute_6  attribute_7  attribute_8  attribute_9  attribute_10  ...  \\\n2         0.2280       0.2431       0.3771       0.5598        0.6194  ...   \n154       0.1267       0.1577       0.1927       0.2361        0.2169  ...   \n183       0.0932       0.0955       0.2140       0.2546        0.2952  ...   \n85        0.1306       0.2112       0.2268       0.2992        0.3735  ...   \n66        0.0438       0.0341       0.0780       0.0844        0.0779  ...   \n..           ...          ...          ...          ...           ...  ...   \n50        0.0792       0.1083       0.0687       0.0298        0.0880  ...   \n184       0.2103       0.2263       0.2524       0.3595        0.5915  ...   \n165       0.0849       0.0812       0.1833       0.2228        0.1810  ...   \n7         0.0922       0.1027       0.0613       0.1465        0.2838  ...   \n70        0.0284       0.0527       0.0575       0.1054        0.1109  ...   \n\n     attribute_51  attribute_52  attribute_53  attribute_54  attribute_55  \\\n2          0.0033        0.0232        0.0166        0.0095        0.0180   \n154        0.0039        0.0053        0.0029        0.0020        0.0013   \n183        0.0310        0.0237        0.0078        0.0144        0.0170   \n85         0.0223        0.0110        0.0071        0.0205        0.0164   \n66         0.0100        0.0038        0.0187        0.0156        0.0068   \n..            ...           ...           ...           ...           ...   \n50         0.0098        0.0163        0.0242        0.0043        0.0202   \n184        0.0346        0.0167        0.0199        0.0145        0.0081   \n165        0.0167        0.0089        0.0051        0.0015        0.0075   \n7          0.0052        0.0081        0.0120        0.0045        0.0121   \n70         0.0023        0.0069        0.0025        0.0027        0.0052   \n\n     attribute_56  attribute_57  attribute_58  attribute_59  attribute_60  \n2          0.0244        0.0316        0.0164        0.0095        0.0078  \n154        0.0029        0.0020        0.0062        0.0026        0.0052  \n183        0.0012        0.0109        0.0036        0.0043        0.0018  \n85         0.0063        0.0078        0.0094        0.0110        0.0068  \n66         0.0097        0.0073        0.0081        0.0086        0.0095  \n..            ...           ...           ...           ...           ...  \n50         0.0108        0.0037        0.0096        0.0093        0.0053  \n184        0.0045        0.0043        0.0027        0.0055        0.0057  \n165        0.0058        0.0016        0.0070        0.0074        0.0038  \n7          0.0097        0.0085        0.0047        0.0048        0.0053  \n70         0.0036        0.0026        0.0036        0.0006        0.0035  \n\n[124 rows x 60 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>attribute_1</th>\n      <th>attribute_2</th>\n      <th>attribute_3</th>\n      <th>attribute_4</th>\n      <th>attribute_5</th>\n      <th>attribute_6</th>\n      <th>attribute_7</th>\n      <th>attribute_8</th>\n      <th>attribute_9</th>\n      <th>attribute_10</th>\n      <th>...</th>\n      <th>attribute_51</th>\n      <th>attribute_52</th>\n      <th>attribute_53</th>\n      <th>attribute_54</th>\n      <th>attribute_55</th>\n      <th>attribute_56</th>\n      <th>attribute_57</th>\n      <th>attribute_58</th>\n      <th>attribute_59</th>\n      <th>attribute_60</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2</th>\n      <td>0.0262</td>\n      <td>0.0582</td>\n      <td>0.1099</td>\n      <td>0.1083</td>\n      <td>0.0974</td>\n      <td>0.2280</td>\n      <td>0.2431</td>\n      <td>0.3771</td>\n      <td>0.5598</td>\n      <td>0.6194</td>\n      <td>...</td>\n      <td>0.0033</td>\n      <td>0.0232</td>\n      <td>0.0166</td>\n      <td>0.0095</td>\n      <td>0.0180</td>\n      <td>0.0244</td>\n      <td>0.0316</td>\n      <td>0.0164</td>\n      <td>0.0095</td>\n      <td>0.0078</td>\n    </tr>\n    <tr>\n      <th>154</th>\n      <td>0.0117</td>\n      <td>0.0069</td>\n      <td>0.0279</td>\n      <td>0.0583</td>\n      <td>0.0915</td>\n      <td>0.1267</td>\n      <td>0.1577</td>\n      <td>0.1927</td>\n      <td>0.2361</td>\n      <td>0.2169</td>\n      <td>...</td>\n      <td>0.0039</td>\n      <td>0.0053</td>\n      <td>0.0029</td>\n      <td>0.0020</td>\n      <td>0.0013</td>\n      <td>0.0029</td>\n      <td>0.0020</td>\n      <td>0.0062</td>\n      <td>0.0026</td>\n      <td>0.0052</td>\n    </tr>\n    <tr>\n      <th>183</th>\n      <td>0.0096</td>\n      <td>0.0404</td>\n      <td>0.0682</td>\n      <td>0.0688</td>\n      <td>0.0887</td>\n      <td>0.0932</td>\n      <td>0.0955</td>\n      <td>0.2140</td>\n      <td>0.2546</td>\n      <td>0.2952</td>\n      <td>...</td>\n      <td>0.0310</td>\n      <td>0.0237</td>\n      <td>0.0078</td>\n      <td>0.0144</td>\n      <td>0.0170</td>\n      <td>0.0012</td>\n      <td>0.0109</td>\n      <td>0.0036</td>\n      <td>0.0043</td>\n      <td>0.0018</td>\n    </tr>\n    <tr>\n      <th>85</th>\n      <td>0.0365</td>\n      <td>0.1632</td>\n      <td>0.1636</td>\n      <td>0.1421</td>\n      <td>0.1130</td>\n      <td>0.1306</td>\n      <td>0.2112</td>\n      <td>0.2268</td>\n      <td>0.2992</td>\n      <td>0.3735</td>\n      <td>...</td>\n      <td>0.0223</td>\n      <td>0.0110</td>\n      <td>0.0071</td>\n      <td>0.0205</td>\n      <td>0.0164</td>\n      <td>0.0063</td>\n      <td>0.0078</td>\n      <td>0.0094</td>\n      <td>0.0110</td>\n      <td>0.0068</td>\n    </tr>\n    <tr>\n      <th>66</th>\n      <td>0.0265</td>\n      <td>0.0440</td>\n      <td>0.0137</td>\n      <td>0.0084</td>\n      <td>0.0305</td>\n      <td>0.0438</td>\n      <td>0.0341</td>\n      <td>0.0780</td>\n      <td>0.0844</td>\n      <td>0.0779</td>\n      <td>...</td>\n      <td>0.0100</td>\n      <td>0.0038</td>\n      <td>0.0187</td>\n      <td>0.0156</td>\n      <td>0.0068</td>\n      <td>0.0097</td>\n      <td>0.0073</td>\n      <td>0.0081</td>\n      <td>0.0086</td>\n      <td>0.0095</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>50</th>\n      <td>0.0353</td>\n      <td>0.0713</td>\n      <td>0.0326</td>\n      <td>0.0272</td>\n      <td>0.0370</td>\n      <td>0.0792</td>\n      <td>0.1083</td>\n      <td>0.0687</td>\n      <td>0.0298</td>\n      <td>0.0880</td>\n      <td>...</td>\n      <td>0.0098</td>\n      <td>0.0163</td>\n      <td>0.0242</td>\n      <td>0.0043</td>\n      <td>0.0202</td>\n      <td>0.0108</td>\n      <td>0.0037</td>\n      <td>0.0096</td>\n      <td>0.0093</td>\n      <td>0.0053</td>\n    </tr>\n    <tr>\n      <th>184</th>\n      <td>0.0269</td>\n      <td>0.0383</td>\n      <td>0.0505</td>\n      <td>0.0707</td>\n      <td>0.1313</td>\n      <td>0.2103</td>\n      <td>0.2263</td>\n      <td>0.2524</td>\n      <td>0.3595</td>\n      <td>0.5915</td>\n      <td>...</td>\n      <td>0.0346</td>\n      <td>0.0167</td>\n      <td>0.0199</td>\n      <td>0.0145</td>\n      <td>0.0081</td>\n      <td>0.0045</td>\n      <td>0.0043</td>\n      <td>0.0027</td>\n      <td>0.0055</td>\n      <td>0.0057</td>\n    </tr>\n    <tr>\n      <th>165</th>\n      <td>0.0221</td>\n      <td>0.0065</td>\n      <td>0.0164</td>\n      <td>0.0487</td>\n      <td>0.0519</td>\n      <td>0.0849</td>\n      <td>0.0812</td>\n      <td>0.1833</td>\n      <td>0.2228</td>\n      <td>0.1810</td>\n      <td>...</td>\n      <td>0.0167</td>\n      <td>0.0089</td>\n      <td>0.0051</td>\n      <td>0.0015</td>\n      <td>0.0075</td>\n      <td>0.0058</td>\n      <td>0.0016</td>\n      <td>0.0070</td>\n      <td>0.0074</td>\n      <td>0.0038</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.0519</td>\n      <td>0.0548</td>\n      <td>0.0842</td>\n      <td>0.0319</td>\n      <td>0.1158</td>\n      <td>0.0922</td>\n      <td>0.1027</td>\n      <td>0.0613</td>\n      <td>0.1465</td>\n      <td>0.2838</td>\n      <td>...</td>\n      <td>0.0052</td>\n      <td>0.0081</td>\n      <td>0.0120</td>\n      <td>0.0045</td>\n      <td>0.0121</td>\n      <td>0.0097</td>\n      <td>0.0085</td>\n      <td>0.0047</td>\n      <td>0.0048</td>\n      <td>0.0053</td>\n    </tr>\n    <tr>\n      <th>70</th>\n      <td>0.0065</td>\n      <td>0.0122</td>\n      <td>0.0068</td>\n      <td>0.0108</td>\n      <td>0.0217</td>\n      <td>0.0284</td>\n      <td>0.0527</td>\n      <td>0.0575</td>\n      <td>0.1054</td>\n      <td>0.1109</td>\n      <td>...</td>\n      <td>0.0023</td>\n      <td>0.0069</td>\n      <td>0.0025</td>\n      <td>0.0027</td>\n      <td>0.0052</td>\n      <td>0.0036</td>\n      <td>0.0026</td>\n      <td>0.0036</td>\n      <td>0.0006</td>\n      <td>0.0035</td>\n    </tr>\n  </tbody>\n</table>\n<p>124 rows × 60 columns</p>\n</div>"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = sonar['Class'] # .target это срез по столбцу target который является идентификатором класса\n",
    "sonar = sonar.drop('Class',axis=1) # .drop - Удаление столбца 'Class', axis=1 - значит удалить столбец, если не указывать явно по умолчанию оно = 0 и удаляет строки из массива\n",
    "X = sonar # Приравняли обновленный массив sonar без столбца классов\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.4, random_state= 40)\n",
    "X_train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "train_labels = to_categorical(y_train)\n",
    "test_labels = to_categorical(y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(32, activation='relu', input_shape=(60,)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(2, activation='softmax'))\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/70\n",
      "13/13 [==============================] - 1s 1ms/step - loss: 0.6830 - accuracy: 0.5806\n",
      "Epoch 2/70\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.6618 - accuracy: 0.6855\n",
      "Epoch 3/70\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.6463 - accuracy: 0.6613\n",
      "Epoch 4/70\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.6358 - accuracy: 0.6855\n",
      "Epoch 5/70\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.6223 - accuracy: 0.6855\n",
      "Epoch 6/70\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.6098 - accuracy: 0.7016\n",
      "Epoch 7/70\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.5942 - accuracy: 0.7500\n",
      "Epoch 8/70\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.5819 - accuracy: 0.7097\n",
      "Epoch 9/70\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.5674 - accuracy: 0.7339\n",
      "Epoch 10/70\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.5621 - accuracy: 0.7258\n",
      "Epoch 11/70\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.5489 - accuracy: 0.7339\n",
      "Epoch 12/70\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.5355 - accuracy: 0.7581\n",
      "Epoch 13/70\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.5303 - accuracy: 0.7339\n",
      "Epoch 14/70\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.5101 - accuracy: 0.7661\n",
      "Epoch 15/70\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4953 - accuracy: 0.8065\n",
      "Epoch 16/70\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.4806 - accuracy: 0.8145\n",
      "Epoch 17/70\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.4806 - accuracy: 0.7581\n",
      "Epoch 18/70\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.4651 - accuracy: 0.8065\n",
      "Epoch 19/70\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.4557 - accuracy: 0.8306\n",
      "Epoch 20/70\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.4367 - accuracy: 0.8387\n",
      "Epoch 21/70\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.4404 - accuracy: 0.7742\n",
      "Epoch 22/70\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.4193 - accuracy: 0.8387\n",
      "Epoch 23/70\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.4194 - accuracy: 0.8065\n",
      "Epoch 24/70\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.4157 - accuracy: 0.8387\n",
      "Epoch 25/70\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.3918 - accuracy: 0.8468\n",
      "Epoch 26/70\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.3844 - accuracy: 0.8629\n",
      "Epoch 27/70\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.3805 - accuracy: 0.8710\n",
      "Epoch 28/70\n",
      "13/13 [==============================] - 0s 376us/step - loss: 0.3719 - accuracy: 0.8306\n",
      "Epoch 29/70\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.3550 - accuracy: 0.8710\n",
      "Epoch 30/70\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.3607 - accuracy: 0.8790\n",
      "Epoch 31/70\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.3518 - accuracy: 0.8710\n",
      "Epoch 32/70\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.3595 - accuracy: 0.8710\n",
      "Epoch 33/70\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.3224 - accuracy: 0.8790\n",
      "Epoch 34/70\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.3267 - accuracy: 0.8468\n",
      "Epoch 35/70\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3330 - accuracy: 0.8548\n",
      "Epoch 36/70\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.3031 - accuracy: 0.8790\n",
      "Epoch 37/70\n",
      "13/13 [==============================] - 0s 751us/step - loss: 0.3082 - accuracy: 0.8871\n",
      "Epoch 38/70\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.3210 - accuracy: 0.8952\n",
      "Epoch 39/70\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.2908 - accuracy: 0.9274\n",
      "Epoch 40/70\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3002 - accuracy: 0.9113\n",
      "Epoch 41/70\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.2697 - accuracy: 0.9113\n",
      "Epoch 42/70\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2948 - accuracy: 0.8952\n",
      "Epoch 43/70\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.2702 - accuracy: 0.9194\n",
      "Epoch 44/70\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.2661 - accuracy: 0.9113\n",
      "Epoch 45/70\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.2549 - accuracy: 0.9274\n",
      "Epoch 46/70\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.2489 - accuracy: 0.9194\n",
      "Epoch 47/70\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2546 - accuracy: 0.9113\n",
      "Epoch 48/70\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.2335 - accuracy: 0.9274\n",
      "Epoch 49/70\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.2370 - accuracy: 0.9194\n",
      "Epoch 50/70\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.2206 - accuracy: 0.9113\n",
      "Epoch 51/70\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.2228 - accuracy: 0.9113\n",
      "Epoch 52/70\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.2221 - accuracy: 0.9113\n",
      "Epoch 53/70\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.2073 - accuracy: 0.9435\n",
      "Epoch 54/70\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.1927 - accuracy: 0.9274\n",
      "Epoch 55/70\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.1982 - accuracy: 0.9194\n",
      "Epoch 56/70\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1873 - accuracy: 0.9355\n",
      "Epoch 57/70\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1891 - accuracy: 0.9194\n",
      "Epoch 58/70\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.1791 - accuracy: 0.9355\n",
      "Epoch 59/70\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.1894 - accuracy: 0.9355\n",
      "Epoch 60/70\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.1667 - accuracy: 0.9435\n",
      "Epoch 61/70\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.1664 - accuracy: 0.9435\n",
      "Epoch 62/70\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1710 - accuracy: 0.9355\n",
      "Epoch 63/70\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.1633 - accuracy: 0.9597\n",
      "Epoch 64/70\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1543 - accuracy: 0.9597\n",
      "Epoch 65/70\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1533 - accuracy: 0.9274\n",
      "Epoch 66/70\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1411 - accuracy: 0.9597\n",
      "Epoch 67/70\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.1396 - accuracy: 0.9435\n",
      "Epoch 68/70\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1384 - accuracy: 0.9435\n",
      "Epoch 69/70\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.1173 - accuracy: 0.9677\n",
      "Epoch 70/70\n",
      "13/13 [==============================] - 0s 584us/step - loss: 0.1363 - accuracy: 0.9597\n"
     ]
    }
   ],
   "source": [
    "model_history = model.fit(X_train, train_labels, epochs=70, batch_size=10, verbose=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 0s/step - loss: 0.3918 - accuracy: 0.8452\n",
      "Test Accuracy:  0.8452380895614624 \n",
      "Test Loss:  0.39181989431381226\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(X_test, test_labels)\n",
    "# Print the test accuracy\n",
    "print('Test Accuracy: ', test_acc, '\\nTest Loss: ', test_loss)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "data": {
      "text/plain": "      Дисперсия_Вейвлета  Ассиметрия_Дисперсия_Вейвлета  Куртоз Вейвлета  \\\n0                3.62160                        8.66610          -2.8073   \n1                4.54590                        8.16740          -2.4586   \n2                3.86600                       -2.63830           1.9242   \n3                3.45660                        9.52280          -4.0112   \n4                0.32924                       -4.45520           4.5718   \n...                  ...                            ...              ...   \n1367             0.40614                        1.34920          -1.4501   \n1368            -1.38870                       -4.87730           6.4774   \n1369            -3.75030                      -13.45860          17.5932   \n1370            -3.56370                       -8.38270          12.3930   \n1371            -2.54190                       -0.65804           2.6842   \n\n      Энтропия изображения  Класс  \n0                 -0.44699      0  \n1                 -1.46210      0  \n2                  0.10645      0  \n3                 -3.59440      0  \n4                 -0.98880      0  \n...                    ...    ...  \n1367              -0.55949      1  \n1368               0.34179      1  \n1369              -2.77710      1  \n1370              -1.28230      1  \n1371               1.19520      1  \n\n[1372 rows x 5 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Дисперсия_Вейвлета</th>\n      <th>Ассиметрия_Дисперсия_Вейвлета</th>\n      <th>Куртоз Вейвлета</th>\n      <th>Энтропия изображения</th>\n      <th>Класс</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3.62160</td>\n      <td>8.66610</td>\n      <td>-2.8073</td>\n      <td>-0.44699</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4.54590</td>\n      <td>8.16740</td>\n      <td>-2.4586</td>\n      <td>-1.46210</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3.86600</td>\n      <td>-2.63830</td>\n      <td>1.9242</td>\n      <td>0.10645</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3.45660</td>\n      <td>9.52280</td>\n      <td>-4.0112</td>\n      <td>-3.59440</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.32924</td>\n      <td>-4.45520</td>\n      <td>4.5718</td>\n      <td>-0.98880</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1367</th>\n      <td>0.40614</td>\n      <td>1.34920</td>\n      <td>-1.4501</td>\n      <td>-0.55949</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1368</th>\n      <td>-1.38870</td>\n      <td>-4.87730</td>\n      <td>6.4774</td>\n      <td>0.34179</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1369</th>\n      <td>-3.75030</td>\n      <td>-13.45860</td>\n      <td>17.5932</td>\n      <td>-2.77710</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1370</th>\n      <td>-3.56370</td>\n      <td>-8.38270</td>\n      <td>12.3930</td>\n      <td>-1.28230</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1371</th>\n      <td>-2.54190</td>\n      <td>-0.65804</td>\n      <td>2.6842</td>\n      <td>1.19520</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>1372 rows × 5 columns</p>\n</div>"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Нейронная сеть на данных банкнот\n",
    "banknote = pd.read_csv('data_banknote_authentication.csv')\n",
    "banknote"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "      Дисперсия_Вейвлета  Ассиметрия_Дисперсия_Вейвлета  Куртоз Вейвлета  \\\n716              0.51950                       -3.26330           3.0895   \n917              1.56310                        0.89599          -1.9702   \n794             -1.62440                       -6.34440           4.6575   \n92               4.30640                        8.20680          -2.7824   \n373              1.91050                        8.87100          -2.3386   \n...                  ...                            ...              ...   \n1016            -2.00420                       -9.36760           9.3333   \n165              4.19270                       -3.26740           2.5839   \n7                2.09220                       -6.81000           8.4636   \n219             -1.30000                       10.26780          -2.9530   \n1350            -0.21888                       -2.20380          -0.0954   \n\n      Энтропия изображения  \n716               -0.98490  \n917                0.65472  \n794                0.16981  \n92                -1.43360  \n373               -0.75604  \n...                    ...  \n1016              -0.10303  \n165                0.21766  \n7                 -0.60216  \n219               -5.86380  \n1350               0.56421  \n\n[823 rows x 4 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Дисперсия_Вейвлета</th>\n      <th>Ассиметрия_Дисперсия_Вейвлета</th>\n      <th>Куртоз Вейвлета</th>\n      <th>Энтропия изображения</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>716</th>\n      <td>0.51950</td>\n      <td>-3.26330</td>\n      <td>3.0895</td>\n      <td>-0.98490</td>\n    </tr>\n    <tr>\n      <th>917</th>\n      <td>1.56310</td>\n      <td>0.89599</td>\n      <td>-1.9702</td>\n      <td>0.65472</td>\n    </tr>\n    <tr>\n      <th>794</th>\n      <td>-1.62440</td>\n      <td>-6.34440</td>\n      <td>4.6575</td>\n      <td>0.16981</td>\n    </tr>\n    <tr>\n      <th>92</th>\n      <td>4.30640</td>\n      <td>8.20680</td>\n      <td>-2.7824</td>\n      <td>-1.43360</td>\n    </tr>\n    <tr>\n      <th>373</th>\n      <td>1.91050</td>\n      <td>8.87100</td>\n      <td>-2.3386</td>\n      <td>-0.75604</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1016</th>\n      <td>-2.00420</td>\n      <td>-9.36760</td>\n      <td>9.3333</td>\n      <td>-0.10303</td>\n    </tr>\n    <tr>\n      <th>165</th>\n      <td>4.19270</td>\n      <td>-3.26740</td>\n      <td>2.5839</td>\n      <td>0.21766</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>2.09220</td>\n      <td>-6.81000</td>\n      <td>8.4636</td>\n      <td>-0.60216</td>\n    </tr>\n    <tr>\n      <th>219</th>\n      <td>-1.30000</td>\n      <td>10.26780</td>\n      <td>-2.9530</td>\n      <td>-5.86380</td>\n    </tr>\n    <tr>\n      <th>1350</th>\n      <td>-0.21888</td>\n      <td>-2.20380</td>\n      <td>-0.0954</td>\n      <td>0.56421</td>\n    </tr>\n  </tbody>\n</table>\n<p>823 rows × 4 columns</p>\n</div>"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = banknote['Класс'] # .target это срез по столбцу target который является идентификатором класса\n",
    "banknote = banknote.drop('Класс',axis=1)\n",
    "X = banknote\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.4, random_state= 40)\n",
    "X_train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[1., 0.],\n       [0., 1.],\n       [0., 1.],\n       ...,\n       [1., 0.],\n       [1., 0.],\n       [0., 1.]], dtype=float32)"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels = to_categorical(y_train)\n",
    "test_labels = to_categorical(y_test)\n",
    "train_labels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "83/83 [==============================] - 1s 1ms/step - loss: 0.6405 - accuracy: 0.6804\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.1945 - accuracy: 0.9478\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.0872 - accuracy: 0.9878\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.0450 - accuracy: 0.9976\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.0268 - accuracy: 0.9976\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.0167 - accuracy: 1.0000\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.0106 - accuracy: 1.0000\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.0074 - accuracy: 1.0000\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.0053 - accuracy: 1.0000\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.0038 - accuracy: 1.0000\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 8.1211e-04 - accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 7.2988e-04 - accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 5.7555e-04 - accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 4.5322e-04 - accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 4.0375e-04 - accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(32, activation='relu', input_shape=(4,)))\n",
    "model.add(layers.Dense(2, activation='softmax'))\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model_history = model.fit(X_train, train_labels, epochs=20, batch_size=10, verbose=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 0s 679us/step - loss: 3.5930e-04 - accuracy: 1.0000\n",
      "Test Accuracy:  1.0 \n",
      "Test Loss:  0.0003592976136133075\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(X_test, test_labels)\n",
    "# Print the test accuracy\n",
    "print('Test Accuracy: ', test_acc, '\\nTest Loss: ', test_loss)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "data": {
      "text/plain": "   alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \\\n0    14.23        1.71  2.43               15.6      127.0           2.80   \n1    13.20        1.78  2.14               11.2      100.0           2.65   \n2    13.16        2.36  2.67               18.6      101.0           2.80   \n3    14.37        1.95  2.50               16.8      113.0           3.85   \n4    13.24        2.59  2.87               21.0      118.0           2.80   \n\n   flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \\\n0        3.06                  0.28             2.29             5.64  1.04   \n1        2.76                  0.26             1.28             4.38  1.05   \n2        3.24                  0.30             2.81             5.68  1.03   \n3        3.49                  0.24             2.18             7.80  0.86   \n4        2.69                  0.39             1.82             4.32  1.04   \n\n   od280/od315_of_diluted_wines  proline  target  \n0                          3.92   1065.0       0  \n1                          3.40   1050.0       0  \n2                          3.17   1185.0       0  \n3                          3.45   1480.0       0  \n4                          2.93    735.0       0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>alcohol</th>\n      <th>malic_acid</th>\n      <th>ash</th>\n      <th>alcalinity_of_ash</th>\n      <th>magnesium</th>\n      <th>total_phenols</th>\n      <th>flavanoids</th>\n      <th>nonflavanoid_phenols</th>\n      <th>proanthocyanins</th>\n      <th>color_intensity</th>\n      <th>hue</th>\n      <th>od280/od315_of_diluted_wines</th>\n      <th>proline</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>14.23</td>\n      <td>1.71</td>\n      <td>2.43</td>\n      <td>15.6</td>\n      <td>127.0</td>\n      <td>2.80</td>\n      <td>3.06</td>\n      <td>0.28</td>\n      <td>2.29</td>\n      <td>5.64</td>\n      <td>1.04</td>\n      <td>3.92</td>\n      <td>1065.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>13.20</td>\n      <td>1.78</td>\n      <td>2.14</td>\n      <td>11.2</td>\n      <td>100.0</td>\n      <td>2.65</td>\n      <td>2.76</td>\n      <td>0.26</td>\n      <td>1.28</td>\n      <td>4.38</td>\n      <td>1.05</td>\n      <td>3.40</td>\n      <td>1050.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>13.16</td>\n      <td>2.36</td>\n      <td>2.67</td>\n      <td>18.6</td>\n      <td>101.0</td>\n      <td>2.80</td>\n      <td>3.24</td>\n      <td>0.30</td>\n      <td>2.81</td>\n      <td>5.68</td>\n      <td>1.03</td>\n      <td>3.17</td>\n      <td>1185.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>14.37</td>\n      <td>1.95</td>\n      <td>2.50</td>\n      <td>16.8</td>\n      <td>113.0</td>\n      <td>3.85</td>\n      <td>3.49</td>\n      <td>0.24</td>\n      <td>2.18</td>\n      <td>7.80</td>\n      <td>0.86</td>\n      <td>3.45</td>\n      <td>1480.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>13.24</td>\n      <td>2.59</td>\n      <td>2.87</td>\n      <td>21.0</td>\n      <td>118.0</td>\n      <td>2.80</td>\n      <td>2.69</td>\n      <td>0.39</td>\n      <td>1.82</td>\n      <td>4.32</td>\n      <td>1.04</td>\n      <td>2.93</td>\n      <td>735.0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Пример по аналогии с ирисами\n",
    "from sklearn.datasets import load_wine\n",
    "wine = load_wine(as_frame= True)\n",
    "wine.frame.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "data": {
      "text/plain": "     alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \\\n66     13.11        1.01  1.70               15.0       78.0           2.98   \n6      14.39        1.87  2.45               14.6       96.0           2.50   \n4      13.24        2.59  2.87               21.0      118.0           2.80   \n15     13.63        1.81  2.70               17.2      112.0           2.85   \n3      14.37        1.95  2.50               16.8      113.0           3.85   \n..       ...         ...   ...                ...        ...            ...   \n12     13.75        1.73  2.41               16.0       89.0           2.60   \n50     13.05        1.73  2.04               12.4       92.0           2.72   \n165    13.73        4.36  2.26               22.5       88.0           1.28   \n7      14.06        2.15  2.61               17.6      121.0           2.60   \n70     12.29        1.61  2.21               20.4      103.0           1.10   \n\n     flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity  \\\n66         3.18                  0.26             2.28             5.30   \n6          2.52                  0.30             1.98             5.25   \n4          2.69                  0.39             1.82             4.32   \n15         2.91                  0.30             1.46             7.30   \n3          3.49                  0.24             2.18             7.80   \n..          ...                   ...              ...              ...   \n12         2.76                  0.29             1.81             5.60   \n50         3.27                  0.17             2.91             7.20   \n165        0.47                  0.52             1.15             6.62   \n7          2.51                  0.31             1.25             5.05   \n70         1.02                  0.37             1.46             3.05   \n\n       hue  od280/od315_of_diluted_wines  proline  \n66   1.120                          3.18    502.0  \n6    1.020                          3.58   1290.0  \n4    1.040                          2.93    735.0  \n15   1.280                          2.88   1310.0  \n3    0.860                          3.45   1480.0  \n..     ...                           ...      ...  \n12   1.150                          2.90   1320.0  \n50   1.120                          2.91   1150.0  \n165  0.780                          1.75    520.0  \n7    1.060                          3.58   1295.0  \n70   0.906                          1.82    870.0  \n\n[106 rows x 13 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>alcohol</th>\n      <th>malic_acid</th>\n      <th>ash</th>\n      <th>alcalinity_of_ash</th>\n      <th>magnesium</th>\n      <th>total_phenols</th>\n      <th>flavanoids</th>\n      <th>nonflavanoid_phenols</th>\n      <th>proanthocyanins</th>\n      <th>color_intensity</th>\n      <th>hue</th>\n      <th>od280/od315_of_diluted_wines</th>\n      <th>proline</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>66</th>\n      <td>13.11</td>\n      <td>1.01</td>\n      <td>1.70</td>\n      <td>15.0</td>\n      <td>78.0</td>\n      <td>2.98</td>\n      <td>3.18</td>\n      <td>0.26</td>\n      <td>2.28</td>\n      <td>5.30</td>\n      <td>1.120</td>\n      <td>3.18</td>\n      <td>502.0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>14.39</td>\n      <td>1.87</td>\n      <td>2.45</td>\n      <td>14.6</td>\n      <td>96.0</td>\n      <td>2.50</td>\n      <td>2.52</td>\n      <td>0.30</td>\n      <td>1.98</td>\n      <td>5.25</td>\n      <td>1.020</td>\n      <td>3.58</td>\n      <td>1290.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>13.24</td>\n      <td>2.59</td>\n      <td>2.87</td>\n      <td>21.0</td>\n      <td>118.0</td>\n      <td>2.80</td>\n      <td>2.69</td>\n      <td>0.39</td>\n      <td>1.82</td>\n      <td>4.32</td>\n      <td>1.040</td>\n      <td>2.93</td>\n      <td>735.0</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>13.63</td>\n      <td>1.81</td>\n      <td>2.70</td>\n      <td>17.2</td>\n      <td>112.0</td>\n      <td>2.85</td>\n      <td>2.91</td>\n      <td>0.30</td>\n      <td>1.46</td>\n      <td>7.30</td>\n      <td>1.280</td>\n      <td>2.88</td>\n      <td>1310.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>14.37</td>\n      <td>1.95</td>\n      <td>2.50</td>\n      <td>16.8</td>\n      <td>113.0</td>\n      <td>3.85</td>\n      <td>3.49</td>\n      <td>0.24</td>\n      <td>2.18</td>\n      <td>7.80</td>\n      <td>0.860</td>\n      <td>3.45</td>\n      <td>1480.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>13.75</td>\n      <td>1.73</td>\n      <td>2.41</td>\n      <td>16.0</td>\n      <td>89.0</td>\n      <td>2.60</td>\n      <td>2.76</td>\n      <td>0.29</td>\n      <td>1.81</td>\n      <td>5.60</td>\n      <td>1.150</td>\n      <td>2.90</td>\n      <td>1320.0</td>\n    </tr>\n    <tr>\n      <th>50</th>\n      <td>13.05</td>\n      <td>1.73</td>\n      <td>2.04</td>\n      <td>12.4</td>\n      <td>92.0</td>\n      <td>2.72</td>\n      <td>3.27</td>\n      <td>0.17</td>\n      <td>2.91</td>\n      <td>7.20</td>\n      <td>1.120</td>\n      <td>2.91</td>\n      <td>1150.0</td>\n    </tr>\n    <tr>\n      <th>165</th>\n      <td>13.73</td>\n      <td>4.36</td>\n      <td>2.26</td>\n      <td>22.5</td>\n      <td>88.0</td>\n      <td>1.28</td>\n      <td>0.47</td>\n      <td>0.52</td>\n      <td>1.15</td>\n      <td>6.62</td>\n      <td>0.780</td>\n      <td>1.75</td>\n      <td>520.0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>14.06</td>\n      <td>2.15</td>\n      <td>2.61</td>\n      <td>17.6</td>\n      <td>121.0</td>\n      <td>2.60</td>\n      <td>2.51</td>\n      <td>0.31</td>\n      <td>1.25</td>\n      <td>5.05</td>\n      <td>1.060</td>\n      <td>3.58</td>\n      <td>1295.0</td>\n    </tr>\n    <tr>\n      <th>70</th>\n      <td>12.29</td>\n      <td>1.61</td>\n      <td>2.21</td>\n      <td>20.4</td>\n      <td>103.0</td>\n      <td>1.10</td>\n      <td>1.02</td>\n      <td>0.37</td>\n      <td>1.46</td>\n      <td>3.05</td>\n      <td>0.906</td>\n      <td>1.82</td>\n      <td>870.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>106 rows × 13 columns</p>\n</div>"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = wine.data # .data это срез по первым 4 столбцам данных длины и ширины\n",
    "y = wine.target # .target это срез по столбцу target который является идентификатором класса\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.4, random_state= 40)\n",
    "X_train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "train_labels = to_categorical(y_train)\n",
    "test_labels = to_categorical(y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(32, activation='relu', input_shape=(13,)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(3, activation='softmax'))\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "11/11 [==============================] - 1s 2ms/step - loss: 15.6293 - accuracy: 0.3113\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 9.2515 - accuracy: 0.4340\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 9.5137 - accuracy: 0.3774\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 8.1633 - accuracy: 0.4151\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 7.1307 - accuracy: 0.4340\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 7.4405 - accuracy: 0.3868\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 9.3353 - accuracy: 0.3868\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 7.6164 - accuracy: 0.4151\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 4.7386 - accuracy: 0.5566\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 5.9876 - accuracy: 0.5377\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 6.1288 - accuracy: 0.4623\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 6.2455 - accuracy: 0.5000\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 6.3008 - accuracy: 0.4906\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 6.3193 - accuracy: 0.5094\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.4708 - accuracy: 0.5377\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 7.0067 - accuracy: 0.4811\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 7.3426 - accuracy: 0.4623\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 4.1236 - accuracy: 0.5283\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.6343 - accuracy: 0.5660\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.7064 - accuracy: 0.4811\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 7.5151 - accuracy: 0.4717\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 6.2718 - accuracy: 0.5000\n",
      "Epoch 23/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 4.3631 - accuracy: 0.5849\n",
      "Epoch 24/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.3675 - accuracy: 0.5094\n",
      "Epoch 25/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.8476 - accuracy: 0.5377\n",
      "Epoch 26/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.1566 - accuracy: 0.5283\n",
      "Epoch 27/100\n",
      "11/11 [==============================] - 0s 901us/step - loss: 5.2904 - accuracy: 0.4811\n",
      "Epoch 28/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 4.0817 - accuracy: 0.5943\n",
      "Epoch 29/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 6.2661 - accuracy: 0.4528\n",
      "Epoch 30/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 4.9472 - accuracy: 0.4906\n",
      "Epoch 31/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.5656 - accuracy: 0.6415\n",
      "Epoch 32/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 4.5997 - accuracy: 0.5566\n",
      "Epoch 33/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 4.7139 - accuracy: 0.5472\n",
      "Epoch 34/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 4.5012 - accuracy: 0.6415\n",
      "Epoch 35/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 4.9184 - accuracy: 0.5849\n",
      "Epoch 36/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 6.1741 - accuracy: 0.4623\n",
      "Epoch 37/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 4.3167 - accuracy: 0.5472\n",
      "Epoch 38/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 4.5630 - accuracy: 0.6038\n",
      "Epoch 39/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.4052 - accuracy: 0.6321\n",
      "Epoch 40/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 4.8529 - accuracy: 0.4906\n",
      "Epoch 41/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 5.5478 - accuracy: 0.5094\n",
      "Epoch 42/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.7531 - accuracy: 0.5094\n",
      "Epoch 43/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.3192 - accuracy: 0.6509\n",
      "Epoch 44/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.0953 - accuracy: 0.6509\n",
      "Epoch 45/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.0671 - accuracy: 0.5943\n",
      "Epoch 46/100\n",
      "11/11 [==============================] - 0s 0s/step - loss: 3.2055 - accuracy: 0.6509\n",
      "Epoch 47/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.6668 - accuracy: 0.5472\n",
      "Epoch 48/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.9880 - accuracy: 0.6887\n",
      "Epoch 49/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.3722 - accuracy: 0.6321\n",
      "Epoch 50/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 4.5247 - accuracy: 0.5849\n",
      "Epoch 51/100\n",
      "11/11 [==============================] - 0s 0s/step - loss: 3.9516 - accuracy: 0.6132\n",
      "Epoch 52/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.9710 - accuracy: 0.5566\n",
      "Epoch 53/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.5789 - accuracy: 0.6698\n",
      "Epoch 54/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.8537 - accuracy: 0.7453\n",
      "Epoch 55/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 4.8355 - accuracy: 0.6038\n",
      "Epoch 56/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.3559 - accuracy: 0.6604\n",
      "Epoch 57/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.7463 - accuracy: 0.5849\n",
      "Epoch 58/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.9361 - accuracy: 0.6321\n",
      "Epoch 59/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 4.0885 - accuracy: 0.5943\n",
      "Epoch 60/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 4.7996 - accuracy: 0.5566\n",
      "Epoch 61/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.7277 - accuracy: 0.6604\n",
      "Epoch 62/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 4.5304 - accuracy: 0.5377\n",
      "Epoch 63/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 2.9651 - accuracy: 0.6415\n",
      "Epoch 64/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 4.0613 - accuracy: 0.6038\n",
      "Epoch 65/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.3058 - accuracy: 0.6981\n",
      "Epoch 66/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.7581 - accuracy: 0.5943\n",
      "Epoch 67/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.2489 - accuracy: 0.7170\n",
      "Epoch 68/100\n",
      "11/11 [==============================] - 0s 701us/step - loss: 2.6524 - accuracy: 0.6415\n",
      "Epoch 69/100\n",
      "11/11 [==============================] - 0s 0s/step - loss: 3.7929 - accuracy: 0.6132\n",
      "Epoch 70/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.7002 - accuracy: 0.6226\n",
      "Epoch 71/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.3710 - accuracy: 0.7075\n",
      "Epoch 72/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.5122 - accuracy: 0.6792\n",
      "Epoch 73/100\n",
      "11/11 [==============================] - 0s 0s/step - loss: 2.6300 - accuracy: 0.6981\n",
      "Epoch 74/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 4.0394 - accuracy: 0.6698\n",
      "Epoch 75/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.3257 - accuracy: 0.6887\n",
      "Epoch 76/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.2179 - accuracy: 0.7547\n",
      "Epoch 77/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 4.5037 - accuracy: 0.5472\n",
      "Epoch 78/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 2.3725 - accuracy: 0.6698\n",
      "Epoch 79/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.9849 - accuracy: 0.7358\n",
      "Epoch 80/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 4.2815 - accuracy: 0.6415\n",
      "Epoch 81/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.6673 - accuracy: 0.5660\n",
      "Epoch 82/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.9563 - accuracy: 0.6321\n",
      "Epoch 83/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.5912 - accuracy: 0.6981\n",
      "Epoch 84/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.1128 - accuracy: 0.7925\n",
      "Epoch 85/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.7443 - accuracy: 0.6509\n",
      "Epoch 86/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.6335 - accuracy: 0.6698\n",
      "Epoch 87/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.1974 - accuracy: 0.7075\n",
      "Epoch 88/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 4.0494 - accuracy: 0.6132\n",
      "Epoch 89/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.5469 - accuracy: 0.6415\n",
      "Epoch 90/100\n",
      "11/11 [==============================] - 0s 0s/step - loss: 3.5697 - accuracy: 0.5849\n",
      "Epoch 91/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.7114 - accuracy: 0.6981\n",
      "Epoch 92/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.7514 - accuracy: 0.7170\n",
      "Epoch 93/100\n",
      "11/11 [==============================] - 0s 801us/step - loss: 2.6021 - accuracy: 0.7358\n",
      "Epoch 94/100\n",
      "11/11 [==============================] - 0s 0s/step - loss: 3.8071 - accuracy: 0.5943\n",
      "Epoch 95/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.0273 - accuracy: 0.7925\n",
      "Epoch 96/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.1102 - accuracy: 0.6132\n",
      "Epoch 97/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.8040 - accuracy: 0.6887\n",
      "Epoch 98/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.2440 - accuracy: 0.6321\n",
      "Epoch 99/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.9405 - accuracy: 0.7453\n",
      "Epoch 100/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.6989 - accuracy: 0.6509\n"
     ]
    }
   ],
   "source": [
    "model_history = model.fit(X_train, train_labels, epochs=100, batch_size=10, verbose=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 0s/step - loss: 0.4789 - accuracy: 0.8750\n",
      "Test Accuracy:  0.875 \n",
      "Test Loss:  0.47894084453582764\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(X_test, test_labels)\n",
    "# Print the test accuracy\n",
    "print('Test Accuracy: ', test_acc, '\\nTest Loss: ', test_loss)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}